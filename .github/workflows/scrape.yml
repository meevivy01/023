name: JobThai Scraper Uni

# Schedule and manual trigger
on:
  workflow_dispatch:       # Manual run button
  schedule:
    - cron: '0 17 * * *'   # Auto-run daily at 00:00 Thailand time (UTC 17:00)

jobs:
  scrape_job:
    # üü¢ ‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô 24.04 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö libasound2t64 ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ
    runs-on: ubuntu-24.04 
    
    # Set Timezone to Thailand
    env:
      TZ: 'Asia/Bangkok'

    steps:
      # 1. Checkout code
      - name: Checkout code
        uses: actions/checkout@v4

      # 2. Set up Python
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      # 3. Install Google Chrome
      - name: Install Google Chrome
        run: |
          wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          sudo sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google-chrome.list'
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable

      # 4. Install Xvfb and Dependencies (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Ubuntu 24.04)
      - name: Install Xvfb and Dependencies
        run: |
          sudo apt-get update
          # libasound2t64 ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Ubuntu 24.04
          sudo apt-get install -y xvfb libnss3 libasound2t64 libgbm1 libu2f-udev fonts-liberation

      # 5. Install Python libraries
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas undetected-chromedriver python-dotenv pyyaml rich thefuzz python-dateutil gspread oauth2client fake-useragent
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      # 6. Create Config Files (‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÑ‡∏ü‡∏•‡πå YAML ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô)
      # ‚ùå ‡∏ï‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á User.env ‡∏≠‡∏≠‡∏Å ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡πà‡∏á‡∏ú‡πà‡∏≤‡∏ô Env Vars ‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô Run ‡πÅ‡∏ó‡∏ô (‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏Å‡∏ß‡πà‡∏≤)
      - name: Create Config Files
        env:
          DATA_COMP: ${{ secrets.COMPETITORS_DATA }}
          DATA_CLIENT: ${{ secrets.CLIENTS_DATA }}
          DATA_TIER1: ${{ secrets.TIER1_DATA }}
        run: |
          echo "$DATA_COMP" > compe.yaml
          echo "$DATA_CLIENT" > co.yaml
          echo "$DATA_TIER1" > tier1.yaml

      # 7. Run Scraper with Xvfb
      - name: Run Scraper with Xvfb
        env:
          # ‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á (Python ‡∏à‡∏∞‡∏≠‡πà‡∏≤‡∏ô‡∏ú‡πà‡∏≤‡∏ô os.getenv ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢)
          JOBTHAI_USER: ${{ secrets.JOBTHAI_USER }}
          JOBTHAI_PASS: ${{ secrets.JOBTHAI_PASS }}
          EMAIL_SENDER: ${{ secrets.EMAIL_SENDER }}
          EMAIL_PASSWORD: ${{ secrets.EMAIL_PASSWORD }}
          EMAIL_RECEIVER: ${{ secrets.EMAIL_RECEIVER }}
          COOKIES_JSON: ${{ secrets.COOKIES_JSON }}
          GITHUB_EVENT_NAME: ${{ github.event_name }}
          G_SHEET_KEY: ${{ secrets.G_SHEET_KEY }}
          G_SHEET_NAME: ${{ secrets.G_SHEET_NAME }}
        run: |
          # üü¢ ‡∏≠‡∏¢‡πà‡∏≤‡∏•‡∏∑‡∏°‡πÄ‡∏ä‡πá‡∏Ñ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ß‡πà‡∏≤‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡πÉ‡∏ô Repo (Git1.py)
          xvfb-run --auto-servernum --server-args="-screen 0 1920x1080x24" python Git1.py

      # 8. Upload Results
      - name: Upload Results (CSV & Images)
        if: always() 
        uses: actions/upload-artifact@v4
        with:
          name: scraper-results
          path: |
            *.csv
            resume_images/
            *.png
            *.json
          retention-days: 5 # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏ô‡πà‡∏≠‡∏¢ ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏°‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á
          
